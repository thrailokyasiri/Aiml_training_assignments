{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thrailokyasiri/Aiml_training_assignments/blob/main/STP_Module_01_Lab_01_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1, Lab 1: Introduction to Machine Learning and Feature Extraction\n",
        "\n",
        "## What is Machine Learning?\n",
        "\n",
        "Machine learning is a branch of artificial intelligence that enables computers to learn patterns from data without being explicitly programmed. Instead of writing specific rules, we train algorithms on examples to make predictions or decisions.\n",
        "\n",
        "### Types of Machine Learning\n",
        "\n",
        "**1. Supervised Learning**\n",
        "- The algorithm learns from labeled data (input-output pairs)\n",
        "- Goal: Learn a mapping function from inputs to outputs\n",
        "- Two main types:\n",
        "  - **Classification**: Predicting categories (e.g., spam/not spam, digit 0-9)\n",
        "  - **Regression**: Predicting continuous values (e.g., house prices, temperature)\n",
        "\n",
        "**2. Unsupervised Learning**\n",
        "- The algorithm finds patterns in unlabeled data\n",
        "- Goal: Discover hidden structure or relationships\n",
        "- Examples: Clustering, dimensionality reduction\n",
        "\n",
        "---\n",
        "\n",
        "**Links**\n",
        "\n",
        "Video - https://youtu.be/hsJidtpHHyo?si=O8Hp2aqvNUELkvmt\n",
        "\n",
        "Text - https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/\n",
        "## Quick ML Examples with Scikit-Learn\n",
        "\n",
        "### Example 1: Classification with Iris Dataset"
      ],
      "metadata": {
        "id": "cc5LIKkVNFpf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6I1yI79fbLD"
      },
      "source": [
        "# Extracting features from data\n",
        "\n",
        "Module 1, Lab 1<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset (flower classification)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Classification Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Predicted classes: {predictions[:5]}\")\n",
        "print(f\"Actual classes: {y_test[:5]}\")"
      ],
      "metadata": {
        "id": "sAsoWNYQNLl-",
        "outputId": "b00ae7f1-6f27-419d-b953-56a337ff7154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 100.00%\n",
            "Predicted classes: [1 0 2 1 1]\n",
            "Actual classes: [1 0 2 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: Regression with Boston Housing Dataset"
      ],
      "metadata": {
        "id": "D2tCiyrGNXUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data[:500], housing.target[:500]  # Use subset for speed\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a regression model\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n",
        "print(f\"\\nFirst 5 predictions: {predictions[:5]}\")\n",
        "print(f\"First 5 actual values: {y_test[:5]}\")"
      ],
      "metadata": {
        "id": "thF4xJuHNVDe",
        "outputId": "872ba18e-5d98-424e-c43c-5404fe99648a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.21\n",
            "R² Score: 0.81\n",
            "\n",
            "First 5 predictions: [2.52559351 0.97113716 1.45014085 3.19893231 1.90248745]\n",
            "First 5 actual values: [2.5   0.675 1.22  4.103 3.357]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Features in Machine Learning\n",
        "\n",
        "### What are Features?\n",
        "\n",
        "**Features** are measurable properties or characteristics of your data that you use as input for machine learning algorithms. Think of them as columns in a spreadsheet.\n",
        "\n",
        "**Example**: If you're predicting house prices:\n",
        "- Features might include: square footage, number of bedrooms, location, age of house\n",
        "- Target/Label: the price (what you want to predict)\n"
      ],
      "metadata": {
        "id": "9iRhzW46Npm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example feature representation\n",
        "house_data = {\n",
        "    'square_feet': [1500, 2000, 1200, 1800],\n",
        "    'bedrooms': [3, 4, 2, 3],\n",
        "    'age_years': [10, 5, 15, 8],\n",
        "    'price': [300000, 450000, 250000, 380000]  # This is the target\n",
        "}"
      ],
      "metadata": {
        "id": "5LJmpiNjNvc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why CSV Format?\n",
        "\n",
        "CSV (Comma-Separated Values) is one of the most common formats for ML datasets because:\n",
        "\n",
        "1. **Structured**: Data is organized in rows and columns (tabular format)\n",
        "2. **Easy to read**: Both humans and machines can parse it easily\n",
        "3. **Universal**: Works with all ML libraries (scikit-learn, pandas, TensorFlow, etc.)\n",
        "\n",
        "**CSV Structure:**\n",
        "```\n",
        "feature1,feature2,feature3,target\n",
        "1.2,3.4,5.6,0\n",
        "2.3,4.5,6.7,1\n",
        "3.4,5.6,7.8,0\n",
        "```\n",
        "\n",
        "Each row = one sample/observation\n",
        "Each column = one feature\n",
        "Last column = target/label (what we want to predict)\n",
        "\n",
        "### Converting Raw Data to Features"
      ],
      "metadata": {
        "id": "IlMB08T0NyAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example: Text data → Features\n",
        "texts = [\"I love this!\", \"Terrible product\", \"Amazing quality\"]\n",
        "\n",
        "# Feature extraction (simple bag-of-words approach)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "text_features = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(\"Text features as array:\")\n",
        "print(text_features.toarray())\n",
        "print(\"\\nFeature names (vocabulary):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Convert to CSV-like format (DataFrame)\n",
        "df = pd.DataFrame(text_features.toarray(),\n",
        "                  columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nAs DataFrame (CSV-like structure):\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "lbK8MC3lN523",
        "outputId": "8f043197-45a8-4cae-ca2a-016d5eadff5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text features as array:\n",
            "[[0 1 0 0 0 1]\n",
            " [0 0 1 0 1 0]\n",
            " [1 0 0 1 0 0]]\n",
            "\n",
            "Feature names (vocabulary):\n",
            "['amazing' 'love' 'product' 'quality' 'terrible' 'this']\n",
            "\n",
            "As DataFrame (CSV-like structure):\n",
            "   amazing  love  product  quality  terrible  this\n",
            "0        0     1        0        0         0     1\n",
            "1        0     0        1        0         1     0\n",
            "2        1     0        0        1         0     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real Example: Iris Dataset as CSV"
      ],
      "metadata": {
        "id": "Re5yfIODN_oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create DataFrame (like a CSV)\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['species'] = iris.target\n",
        "\n",
        "print(\"Iris dataset in CSV-like format:\")\n",
        "print(df_iris.head())\n",
        "\n",
        "# Save as actual CSV\n",
        "df_iris.to_csv('iris_dataset.csv', index=False)\n",
        "print(\"\\n✓ Saved as iris_dataset.csv\")"
      ],
      "metadata": {
        "id": "WkL_AwUSOBy6",
        "outputId": "d97391c4-a141-4333-b4a7-181c536f24d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset in CSV-like format:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   species  \n",
            "0        0  \n",
            "1        0  \n",
            "2        0  \n",
            "3        0  \n",
            "4        0  \n",
            "\n",
            "✓ Saved as iris_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimBnfcpvcNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677adca5-d0c5-46f9-ad64-6931ccecb43a"
      },
      "source": [
        "! pip install wikipedia\n",
        "\n",
        "import wikipedia\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=c714f58a0789f14d2fef3a31d2f8791f43424b8ed25ecd8f8444556e47e471e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6hGhIGiy4GP"
      },
      "source": [
        "# Part 1: Features of text\n",
        "How do we apply machine learning on text? We can't directly use the text as input to our algorithms. We need to convert them to features. In this notebook, we will explore a simple way of converting text to features.\n",
        "\n",
        "Let us download a few documents off Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpUmCoEr2R3J"
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "# Fix for Wikipedia PageError\n",
        "topic1 = 'Giraffe'\n",
        "topic2 = 'Elephant'\n",
        "\n",
        "def get_wikipedia_content(topic, lang='en'):\n",
        "    \"\"\"Safely fetch Wikipedia content with error handling\"\"\"\n",
        "    wikipedia.set_lang(lang)\n",
        "    try:\n",
        "        # Disable auto-suggest to prevent title mismatches\n",
        "        page = wikipedia.page(topic, auto_suggest=False)\n",
        "        return page.content\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        # If multiple pages exist, use the first option\n",
        "        print(f\"Disambiguation for '{topic}': using '{e.options[0]}'\")\n",
        "        return wikipedia.page(e.options[0]).content\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        # If page not found, try with auto-suggest enabled\n",
        "        print(f\"Page not found for '{topic}', trying auto-suggest...\")\n",
        "        return wikipedia.page(topic).content\n",
        "\n",
        "# Fetch content\n",
        "eng1 = get_wikipedia_content(topic1, 'en')\n",
        "eng2 = get_wikipedia_content(topic2, 'en')\n",
        "fr1 = get_wikipedia_content(topic1, 'fr')\n",
        "fr2 = get_wikipedia_content(topic2, 'fr')\n",
        "\n",
        "print(\"✓ Successfully fetched all Wikipedia pages\")\n",
        "print(f\"English {topic1} length: {len(eng1)} characters\")\n",
        "print(f\"English {topic2} length: {len(eng2)} characters\")\n",
        "print(f\"French {topic1} length: {len(fr1)} characters\")\n",
        "print(f\"French {topic2} length: {len(fr2)} characters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7RlhMiO5kd"
      },
      "source": [
        "This is what the text looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW0G-t912UXZ"
      },
      "source": [
        "fr2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZkmNJ7XO9xX"
      },
      "source": [
        "We need to clean this up a bit. Let us remove all the special characters and keep only 26 letters and space. Note that this will remove accented characters in French also. We are also removing all the numbers and spaces. So this is not an ideal solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5yf5P9pPI4t"
      },
      "source": [
        "def cleanup(text):\n",
        "  text = text.lower()  # make it lowercase\n",
        "  text = re.sub('[^a-z]+', '', text) # only keep characters\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOjC32fRuTK"
      },
      "source": [
        "eng1 = cleanup(eng1)\n",
        "eng2 = cleanup(eng2)\n",
        "fr1 = cleanup(fr1)\n",
        "fr2 = cleanup(fr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIdqvL2G-LqL"
      },
      "source": [
        "print(eng1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFTWwd0rk63"
      },
      "source": [
        "Now let us calculate the frequency of the character n-grams. N-grams are groups of characters of size n. A unigram is a single character and a bigram is a group of two characters and so on.\n",
        "\n",
        "Let us count the frequency of each character in a text and plot it in a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Lz3YUjN0L5"
      },
      "source": [
        "# convert a tuple of characters to a string\n",
        "def tuple2string(tup):\n",
        "  st = ''\n",
        "  for ii in tup:\n",
        "    st = st + ii\n",
        "  return st\n",
        "\n",
        "# convert a tuple of tuples to a list of strings\n",
        "def key2string(keys):\n",
        "  return [tuple2string(i) for i in keys]\n",
        "\n",
        "# plot the histogram\n",
        "def plothistogram(ngram):\n",
        "  keys = key2string(ngram.keys())\n",
        "  values = list(ngram.values())\n",
        "\n",
        "  # sort the keys in alphabetic order\n",
        "  combined = zip(keys, values)\n",
        "  zipped_sorted = sorted(combined, key=lambda x: x[0])\n",
        "  keys, values = map(list, zip(*zipped_sorted))\n",
        "  plt.bar(keys, values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHD62zbZcwAB"
      },
      "source": [
        "Let us compare the histograms of English pages and French pages. Can you spot a difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKcGRgH6b0KP"
      },
      "source": [
        "unigram_eng1 = Counter(ngrams(eng1,1))\n",
        "plothistogram(unigram_eng1)\n",
        "plt.title('English 1')\n",
        "plt.show()\n",
        "unigram_eng2 = Counter(ngrams(eng2,1))\n",
        "plothistogram(unigram_eng2)\n",
        "plt.title('English 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDM_UhCL2QLt"
      },
      "source": [
        "unigram_fr1 = Counter(ngrams(fr1,1))\n",
        "plothistogram(unigram_eng1)\n",
        "plt.title('French 1')\n",
        "plt.show()\n",
        "unigram_fr2 = Counter(ngrams(fr2,1))\n",
        "plothistogram(unigram_fr2)\n",
        "plt.title('French 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxgrdZLKdkAB"
      },
      "source": [
        "We can see that the unigrams for French and English are very similar. So this is not a good feature if we want to distinguish between English and French. Let us look at bigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmRCxItx2T9W"
      },
      "source": [
        "bigram_eng1 = Counter(ngrams(eng1,2)) # bigrams\n",
        "plothistogram(bigram_eng1)\n",
        "plt.title('English 1')\n",
        "plt.show()\n",
        "\n",
        "bigram_eng2 = Counter(ngrams(eng2,2))\n",
        "plothistogram(bigram_eng2)\n",
        "plt.title('English 2')\n",
        "plt.show()\n",
        "\n",
        "bigram_fr1 = Counter(ngrams(fr1,2))\n",
        "plothistogram(bigram_eng1)\n",
        "plt.title('French 1')\n",
        "plt.show()\n",
        "\n",
        "bigram_fr2 = Counter(ngrams(fr2,2))\n",
        "plothistogram(bigram_fr2)\n",
        "plt.title('French 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-egsHMIg5Rp"
      },
      "source": [
        "Another way to visualize bigrams is to use a 2-dimensional graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EaPJgtaVxZM"
      },
      "source": [
        "def plotbihistogram(ngram):\n",
        "  freq = np.zeros((26,26))\n",
        "  for ii in range(26):\n",
        "    for jj in range(26):\n",
        "      freq[ii,jj] = ngram[(chr(ord('a')+ii), chr(ord('a')+jj))]\n",
        "  plt.imshow(freq, cmap = 'jet')\n",
        "  return freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7jq3AwnVzQT"
      },
      "source": [
        "bieng1 = plotbihistogram(bigram_eng1)\n",
        "plt.show()\n",
        "bieng2 = plotbihistogram(bigram_eng2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXPTOj67WsPT"
      },
      "source": [
        "bifr1 = plotbihistogram(bigram_fr1)\n",
        "plt.show()\n",
        "bifr2 = plotbihistogram(bigram_fr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGOEHcyGokD0"
      },
      "source": [
        "Let us look at the top 10 ngrams for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk2TkzTno8vb"
      },
      "source": [
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "def ind2tup(ind):\n",
        "  ind = int(ind)\n",
        "  i = int(ind/26)\n",
        "  j = int(ind%26)\n",
        "  return (chr(ord('a')+i), chr(ord('a')+j))\n",
        "\n",
        "def ShowTopN(bifreq, n=10):\n",
        "  f = bifreq.flatten()\n",
        "  arg = np.argsort(-f)\n",
        "  for ii in range(n):\n",
        "    print(f'{ind2tup(arg[ii])} : {f[arg[ii]]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HeWNh_q0QZ1"
      },
      "source": [
        "print('\\nEnglish 1:')\n",
        "ShowTopN(bieng1)\n",
        "print('\\nEnglish 2:')\n",
        "ShowTopN(bieng2)\n",
        "print('\\nFrench 1:')\n",
        "ShowTopN(bifr1)\n",
        "print('\\nFrench 2:')\n",
        "ShowTopN(bifr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDovOP4l98z"
      },
      "source": [
        "We observe that the bigrams are similar across different topics but different across languages. Thus, the bigram frequency is a good feature for distinguishing languages, but not for distinguishing topics.\n",
        "\n",
        "Thus, we were able to convert a many-dimensional input (the text) to 26 dimesions (unigrams) or 26*26 dimensions (bigrams).\n",
        "\n",
        "\n",
        "A few ways to explore:\n",
        "1. Try with different languages.\n",
        "2. The topics we used are quite similar, wikipedia articles of 'elephant' and 'giraffe'. What happens if we use very different topics? What if we use text from another source than Wikipedia?\n",
        "3. How can we use and visualize trigrams and higher n-grams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJfjIHk-oHV"
      },
      "source": [
        "# Part 2: Written numbers\n",
        "\n",
        "We will use a subset of the MNIST dataset. Each input character is represented in a 28*28 array. Let us see if we can extract some simple features from these images which can help us distinguish between the digits.\n",
        "\n",
        "Load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNsLJSr6wGY0"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVNr144WAUZO"
      },
      "source": [
        "Extract a subset of the data for our experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3MN8ddxAASZ"
      },
      "source": [
        "no1 = train_X[train_y==1,:,:]\n",
        "no0 = train_X[train_y==0,:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePXCs0qyCLpc"
      },
      "source": [
        "Let us visualize a few images here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQeyZSh-Arpc"
      },
      "source": [
        "for ii in range(5):\n",
        "  plt.subplot(1, 5, ii+1)\n",
        "  plt.imshow(no1[ii,:,:])\n",
        "plt.show()\n",
        "for ii in range(5):\n",
        "  plt.subplot(1, 5, ii+1)\n",
        "  plt.imshow(no0[ii,:,:])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-Tg7EKDz96"
      },
      "source": [
        "suNow, let us start with a simple feature: the sum of all pixels and see how good this feature is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8SztDk7CyZc"
      },
      "source": [
        "sum1 = np.sum(no1>0, (1,2)) # threshold before adding up\n",
        "sum0 = np.sum(no0>0, (1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oW3XCOCE7Zv"
      },
      "source": [
        "Let us visualize how good this feature is: (X-axis is mean, y-axis is the digit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8PIe8o_DPpU"
      },
      "source": [
        "plt.hist(sum1, alpha=0.7);\n",
        "plt.hist(sum0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_hToEepFtl2"
      },
      "source": [
        "We can already see that this feature separates the two classes quite well.\n",
        "\n",
        "Let us look at another, more complicated feature. We will count the number black pixels that are surrounded on four sides by non-black pixels, or \"hole pixels\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwEnlm6RFFej"
      },
      "source": [
        "def cumArray(img):\n",
        "  img2 = img.copy()\n",
        "  for ii in range(1, img2.shape[1]):\n",
        "    img2[ii,:] = img2[ii,:] + img2[ii-1,:]  # for every row, add up all the rows above it.\n",
        "  img2 = img2>0\n",
        "  return img2\n",
        "\n",
        "def getHolePixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  hole = hull & ~ (img>0) # remove the original digit to leave behind the holes\n",
        "  return hole"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw3HjgnupUEI"
      },
      "source": [
        "Visualize a few:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0sjr23NYEFe"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getHolePixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS-4erNXtxMi"
      },
      "source": [
        "Now let us plot the number of hole pixels and see how this feature behaves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpm1dRgsety8"
      },
      "source": [
        "hole1 = np.array([getHolePixels(i).sum() for i in no1])\n",
        "hole0 = np.array([getHolePixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hole1, alpha=0.7);\n",
        "plt.hist(hole0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjCBHpJ31yq"
      },
      "source": [
        "This feature works even better to distinguish between one and zero.\n",
        "\n",
        "\n",
        "Now let us try the number of pixels in the 'hull' or the number with the holes filled in:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPtJ8eqolAOf"
      },
      "source": [
        "def getHullPixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  return hull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3fOgyYjmJ48"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getHullPixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5rHal_HRWnE"
      },
      "source": [
        "Plotting the number of hull pixels versus the digit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTLzYZLTRQ_p"
      },
      "source": [
        "hull1 = np.array([getHullPixels(i).sum() for i in no1])\n",
        "hull0 = np.array([getHullPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(hull1, alpha=0.7);\n",
        "plt.hist(hull0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSzH26ElXNri"
      },
      "source": [
        "Let us try one more feature, where we look at the number of boundary pixels in each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2czBypXMwT"
      },
      "source": [
        "def minus(a, b):\n",
        "  return a & ~ b\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "  img = img.copy()>0  # binarize the image\n",
        "  rshift = np.roll(img, 1, 1)\n",
        "  lshift = np.roll(img, -1 ,1)\n",
        "  ushift = np.roll(img, -1, 0)\n",
        "  dshift = np.roll(img, 1, 0)\n",
        "  boundary = minus(img, rshift) | minus(img, lshift) | minus(img, ushift) | minus(img, dshift)\n",
        "  return boundary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-V688jFerXh"
      },
      "source": [
        "imgs = [no1[456,:,:],  no0[456,:,:]]\n",
        "for img in imgs:\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(getBoundaryPixels(img))\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSsxsbCNXcNh"
      },
      "source": [
        "bound1 = np.array([getBoundaryPixels(i).sum() for i in no1])\n",
        "bound0= np.array([getBoundaryPixels(i).sum() for i in no0])\n",
        "\n",
        "plt.hist(bound1, alpha=0.7);\n",
        "plt.hist(bound0, alpha=0.7);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuP04Ao_R0Yz"
      },
      "source": [
        "What will happen if we plot two features together?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl7xWg-WRkAy"
      },
      "source": [
        "# Sum and hull\n",
        "plt.scatter(sum0, hull0, alpha=0.1)\n",
        "plt.scatter(sum1, hull1, alpha=0.1)\n",
        "plt.xlabel('Sum')\n",
        "plt.ylabel('Hull')\n",
        "plt.legend(['0','1'])\n",
        "plt.show()\n",
        "\n",
        "# Sum and hole\n",
        "plt.scatter(sum0, hole0, alpha=0.1)\n",
        "plt.scatter(sum1, hole1, alpha=0.1)\n",
        "plt.xlabel('Sum');\n",
        "plt.ylabel('Hole');\n",
        "plt.legend(['0','1'])\n",
        "plt.show()\n",
        "\n",
        "# Hole and boundary\n",
        "plt.scatter(bound0, hole0, alpha=0.1)\n",
        "plt.scatter(bound1, hole1, alpha=0.1)\n",
        "plt.xlabel('Boundary');\n",
        "plt.ylabel('Hole');\n",
        "plt.legend(['0','1'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JYLmKNFSIT-"
      },
      "source": [
        "Now let us try plotting 3 features together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOKEHIXFaWp_"
      },
      "source": [
        "cl1 = ['class 1']*len(sum1)\n",
        "cl0 = ['class 0']*len(sum0)\n",
        "df = pd.DataFrame(list(zip(np.concatenate((hole0, hole0)), np.concatenate((sum1,sum0)),\n",
        "                           np.concatenate((bound1,bound0)), np.concatenate((cl1, cl0)))),\n",
        "               columns =['Hole', 'Sum', 'Boundary', 'Class'])\n",
        "df.head()\n",
        "fig = px.scatter_3d(df, x='Hole', y='Sum', z='Boundary', color='Class', opacity=0.1)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDGHlFSd5Fu"
      },
      "source": [
        "Feel free to explore the above graph with your mouse.\n",
        "\n",
        "\n",
        "We have seen that we extracted four features from a 28*28 dimensional image.\n",
        "\n",
        "\n",
        "Some questions to explore:\n",
        "1. Which is the best combination of features?\n",
        "2. How would you test or visualize four or more features?\n",
        "3. Can you come up with your own features?\n",
        "4. Will these features work for different classes other than 0 and 1?\n",
        "5. What will happen if we take more that two classes at a time?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The best combination of features is hole pixels, boundary pixels, and pixel sum. These features complement each other and together provide better separation between classes compared to using any single feature alone.\n",
        "2.Four or more features can be tested or visualized using:\n",
        "Pairwise scatter plots\n",
        "3D and higher-dimensional plots\n",
        "Dimensionality reduction techniques such as PCA or t-SNE\n",
        "Parallel coordinate plots\n",
        "These methods help in understanding feature relationships and class separation.\n",
        "3.Yes, additional features can be designed based on the problem. Examples include:\n",
        "Aspect ratio of the digit\n",
        "Stroke width\n",
        "Number of endpoints\n",
        "Vertical or horizontal symmetry\n",
        "Center of mass of pixels\n",
        "Custom features can improve model performance.\n",
        "4.Some features like pixel sum and boundary pixels can work for other digits. However, features such as hole pixels may not be effective for digits that do not contain loops, such as 2, 3, or 5.\n",
        "5.When more than two classes are considered:\n",
        "Feature overlap increases\n",
        "Class separation becomes harder\n",
        "More features or advanced classifiers are required\n",
        "Visualization becomes more complex\n",
        "Using multiple features and better models helps handle multi-class problems."
      ],
      "metadata": {
        "id": "Gi6k4F8hGG3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6UCoZF6AHNnN"
      }
    }
  ]
}